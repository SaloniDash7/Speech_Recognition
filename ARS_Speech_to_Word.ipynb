{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from numpy import matrix\n",
    "from tensorflow.python.ops import ctc_ops as ctc\n",
    "\n",
    "def read_data():\n",
    "    text_files = [os.path.join(root, name) for root, dirs, files in os.walk('Text_Targets') for name in files\n",
    "                 if name.endswith((\".npy\"))]\n",
    "    full_text=\"\"\n",
    "    for file in text_files:\n",
    "        count=0\n",
    "        text_No_Suffix=os.path.splitext(file)[0]\n",
    "        text_File_Name=text_No_Suffix.split('\\\\')[-1]\n",
    "        content=str(np.load(\".\\\\Text_Targets\\\\\"+text_File_Name+'.npy'))\n",
    "        content = io.StringIO(content).readlines()\n",
    "        content = ((content[0].replace(\"\\n\",\"\")).replace(\".\",\"\")).replace(\"?\", \"\")\n",
    "        full_text+=(' '+content)\n",
    "    full_text = full_text.split()\n",
    "    full_text = np.array(full_text)\n",
    "    full_text = np.reshape(full_text,[-1,])\n",
    "    return full_text\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()  #Returns a dictionary with count of each word arranged in descending order of count\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary) #Basically assigns a number to each word \n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "data=read_data()\n",
    "dictionary,reverse_dictionary=build_dataset(data)\n",
    "\n",
    "n_classes=len(dictionary)+2\n",
    "n_features=13\n",
    "#Hyperparameters\n",
    "num_layers=1\n",
    "n_hidden=100\n",
    "batch_size=1\n",
    "n_epochs=1\n",
    "\n",
    "#Target log path\n",
    "logs_path = '/tmp/tensorflow/timit_speech_recognition'\n",
    "\n",
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "def sparse_tuple_from(sequences, dtype=np.int32):\n",
    "    \"\"\"Create a sparse representention of x.\n",
    "    Args:\n",
    "        sequences: a list of lists of type dtype where each element is a sequence\n",
    "    Returns:\n",
    "        A tuple with (indices, values, shape)\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    values = []\n",
    "\n",
    "    for n, seq in enumerate(sequences):\n",
    "        indices.extend(zip([n] * len(matrix(seq)), range(len(matrix(seq)))))\n",
    "        values.extend([seq])\n",
    "\n",
    "    indices = np.asarray(indices, dtype=np.int32)\n",
    "    indices[:,[0, 1]] = indices[:,[1, 0]]\n",
    "    values = np.asarray(values, dtype=dtype)\n",
    "    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1] + 1], dtype=np.int32)\n",
    "\n",
    "    return indices, values, shape\n",
    "\n",
    "\n",
    "inputs = tf.placeholder(tf.float32,[None,None,n_features])\n",
    "target_idx = tf.placeholder(tf.int64)\n",
    "target_vals = tf.placeholder(tf.int32)\n",
    "target_shape = tf.placeholder(tf.int64)\n",
    "targets = tf.SparseTensor(target_idx, target_vals, target_shape)\n",
    "seq_len = tf.placeholder(tf.int32)\n",
    "\n",
    "# RNN output node weights and biases\n",
    "weights = {\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden,n_classes]),dtype=tf.float32) # Weights_shape = hidden_units X vocab_size\n",
    "    }\n",
    "biases = {\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]),tf.float32)\n",
    "        }\n",
    "        \n",
    "def LSTM_cell():\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, reuse=tf.get_variable_scope().reuse)\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=0.4)\n",
    "\n",
    "def RNN_Model(inputs,seq_len,weights,biases):\n",
    "    rnn_cell=LSTM_cell()\n",
    "    #rnn_cell = tf.contrib.rnn.MultiRNNCell(LSTM_cell() for i in range(num_layers))\n",
    "    outputs,_ = tf.nn.dynamic_rnn(rnn_cell,inputs,seq_len,dtype=tf.float32)\n",
    "        \n",
    "    outputs = tf.reshape(outputs,[-1,n_hidden])\n",
    "    #print(outputs)\n",
    "    logits = tf.matmul(outputs,weights['out']) + biases['out']\n",
    "        \n",
    "    with tf.name_scope('Weights'):\n",
    "        variable_summaries(weights['out'])\n",
    "    \n",
    "    with tf.name_scope('Biases'):\n",
    "        variable_summaries(biases['out'])\n",
    "    \n",
    "    with tf.name_scope('Activations'):\n",
    "        tf.summary.histogram('Activations',logits)\n",
    "        \n",
    "    logits = tf.reshape(logits,[batch_size,-1,n_classes])\n",
    "    logits = tf.transpose(logits,(1,0,2))\n",
    "    return logits\n",
    "\n",
    "logits=RNN_Model(inputs,seq_len,weights,biases)\n",
    "loss = ctc.ctc_loss(targets,logits,seq_len)\n",
    "with tf.name_scope(\"CTC_Loss\"):\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    tf.summary.scalar('CTC_Loss',cost)\n",
    "        \n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=0.005, momentum=0.9).minimize(cost)\n",
    "decoded, log_prob = ctc.ctc_beam_search_decoder(logits, seq_len)\n",
    "        \n",
    "label_error_rate = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32),\n",
    "                                              targets)) \n",
    "#computes the Levenshtein distance between sequences.\n",
    "#This operation takes variable-length sequences (hypothesis and truth), each provided as a SparseTensor, and computes the Levenshtein distance.\n",
    "# You can normalize the edit distance by length of truth by setting normalize to true.\n",
    "\n",
    "def convert_to_sequence(values):\n",
    "    result=\"\"\n",
    "    for value in values:\n",
    "        result+=' '+reverse_dictionary[value]\n",
    "    return result\n",
    "        \n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(logs_path)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "def train():\n",
    "    with tf.Session(config=config) as sess:\n",
    "            sess.run(init)\n",
    "            writer.add_graph(sess.graph)\n",
    "            \n",
    "            mfcc_files = [os.path.join(root, name) for root, dirs, files in os.walk('mfccnpyFiles') for name in files\n",
    "             if name.endswith((\".npy\"))]\n",
    "            text_targets=[os.path.join(root, name) for root, dirs, files in os.walk('Text_Targets') for name in files\n",
    "             if name.endswith((\".npy\"))]\n",
    "            for curr_epoch in range(n_epochs):\n",
    "                train_cost=train_ler=0\n",
    "                count_files=0\n",
    "                for file in mfcc_files:\n",
    "                    count=0\n",
    "                    mfcc_No_Suffix=os.path.splitext(file)[0]\n",
    "                    mfcc_File_Name=mfcc_No_Suffix.split('\\\\')[-1]\n",
    "                    for text in text_targets:\n",
    "                        text_No_Suffix=os.path.splitext(text)[0]\n",
    "                        text_File_Name=text_No_Suffix.split('\\\\')[-1]\n",
    "                        if(mfcc_File_Name==text_File_Name):\n",
    "                            count+=1\n",
    "                            train_inputs=np.load(\".\\\\mfccnpyFiles\\\\\"+mfcc_File_Name+'.npy')\n",
    "                            train_inputs = np.asarray(train_inputs[np.newaxis, :])\n",
    "                            train_inputs = (train_inputs - np.mean(train_inputs)) / np.std(train_inputs)\n",
    "                            train_seq_len = [train_inputs.shape[1]]\n",
    "                            #print(train_seq_len)\n",
    "                            \n",
    "                            content=str(np.load(\".\\\\Text_Targets\\\\\"+text_File_Name+'.npy'))\n",
    "                            content = io.StringIO(content).readlines()\n",
    "                            content = ((content[0].replace(\"\\n\",\"\")).replace(\".\",\"\")).replace(\"?\", \"\")\n",
    "                            content = content.split()\n",
    "                            train_targets=[]\n",
    "                            for word in content:\n",
    "                                train_targets.append(dictionary[word])\n",
    "                            train_targets=np.array(train_targets)\n",
    "                            \n",
    "                            target_index,target_values,target_shapes = sparse_tuple_from(train_targets)\n",
    "                            feed = {inputs: train_inputs,target_idx:target_index,target_vals:target_values,target_shape:target_shapes,seq_len: train_seq_len}\n",
    "                            summary,batch_cost, _ = sess.run([merged, cost, optimizer], feed)\n",
    "                            writer.add_summary(summary, count_files)\n",
    "                            \n",
    "                            train_cost += batch_cost * batch_size\n",
    "                            print('Truth:\\n' + convert_to_sequence(train_targets))\n",
    "                            print('Output:\\n' + convert_to_sequence(sess.run(decoded[0].values,feed_dict=feed)))\n",
    "                            #print(\"Decoded Values: \" ,sess.run(decoded[0].values,feed_dict=feed))\n",
    "                            train_ler += sess.run(label_error_rate, feed_dict=feed) * batch_size\n",
    "                            count_files+=1\n",
    "                            print(\"Batch Cost {0} after {1} file \\n\".format(batch_cost,count_files))\n",
    "                            if(count_files==6300):\n",
    "                                train_cost=(train_cost/6300)\n",
    "                                tf.summary.scalar('Train_Cost',cost)\n",
    "                                writer.add_summary(summary, curr_epoch)\n",
    "                            \n",
    "                            \n",
    "                        if(count>=1):\n",
    "                            break\n",
    "                \n",
    "                print(\" Train Cost: {0} in Epoch {1} \".format((train_cost),curr_epoch))\n",
    "                saver.save(sess,\"./saver/model.ckpt\")\n",
    "                print(\"Model saved \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth:\n",
      " She had your dark suit in greasy wash water all year\n",
      "Output:\n",
      " greasy units, greasy anticipated greasy morrow greasy supporters greasy provocatively greasy reasons greasy tied greasy husband's greasy exactly greasy\n",
      "Batch Cost 759.9910278320312 after 1 file \n",
      "\n",
      "Truth:\n",
      " Don't ask me to carry an oily rag like that\n",
      "Output:\n",
      " points stew intuition fathers convincing compose witches vacation sophistication stew diet seller storyline, lobes wieners slipping welcomed hope rocky Milk premiums fit no overcharged prepared sugar radicalism see endosperm heaven myopia cooperative intuition diet unit realize stew farmyard senate monopolize oily senate prayed lobes welcomed sighing, mine generously interested, Tradition\n",
      "Batch Cost 2278.267333984375 after 2 file \n",
      "\n",
      "Truth:\n",
      " Bricks are an alternative\n",
      "Output:\n",
      " predominantly points endosperm morale slipping meet\n",
      "Batch Cost 346.4275817871094 after 3 file \n",
      "\n",
      "Truth:\n",
      " Fat showed in loose rolls beneath the shirt\n",
      "Output:\n",
      " them\n",
      "Batch Cost 143.28076171875 after 4 file \n",
      "\n",
      "Truth:\n",
      " It suffers from a lack of unity of purpose and respect for heroic leadership\n",
      "Output:\n",
      "\n",
      "Batch Cost 307.34033203125 after 5 file \n",
      "\n",
      "Truth:\n",
      " She slipped and sprained her ankle on the steep slope\n",
      "Output:\n",
      "\n",
      "Batch Cost 227.7292938232422 after 6 file \n",
      "\n",
      "Truth:\n",
      " Aluminum silverware can often be flimsy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-93fd337a0d5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-468038de71e7>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    195\u001b[0m                             \u001b[0mtrain_cost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_cost\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Truth:\\n'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mconvert_to_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_targets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m                             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output:\\n'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mconvert_to_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m                             \u001b[1;31m#print(\"Decoded Values: \" ,sess.run(decoded[0].values,feed_dict=feed))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m                             \u001b[0mtrain_ler\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_error_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\saloni dash\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\saloni dash\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\saloni dash\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mc:\\users\\saloni dash\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\saloni dash\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
