{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import io\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from numpy import matrix\n",
    "from tensorflow.python.ops import ctc_ops as ctc\n",
    "import glob\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--action\")\n",
    "parser.add_argument(\"--epoch\",default=0)\n",
    "parser.parse_args()\n",
    "args = parser.parse_args()\n",
    "print(int(args.epoch))\n",
    "def read_data():\n",
    "    text_files = [os.path.join(root, name) for root, dirs, files in os.walk('Text_Targets') for name in files\n",
    "                 if name.endswith((\".npy\"))]\n",
    "    full_text=\"\"\n",
    "    for file in text_files:\n",
    "        count=0\n",
    "        text_No_Suffix=os.path.splitext(file)[0]\n",
    "        text_File_Name=text_No_Suffix.split('\\\\')[-1]\n",
    "        content=str(np.load(\".\\\\Text_Targets\\\\\"+text_File_Name+'.npy'))\n",
    "        content = io.StringIO(content).readlines()\n",
    "        content = (((((content[0].replace(\"\\n\",\"\")).replace(\".\",\"\")).replace(\"?\", \"\")).replace('\"','')).replace(\"!\",\"\")).replace(\",\",\"\")\n",
    "        content = (((content.replace(\":\",\"\")).replace(\"'\",\"\")).replace(\"-\",\"\")).replace(\";\",\"\")\n",
    "        full_text+=(' '+content)\n",
    "    return full_text.lower()\n",
    "\n",
    "def build_dataset(words):\n",
    "    list_val=list(set(words))\n",
    "    dictionary = dict()\n",
    "    for word in list_val:\n",
    "        dictionary[word] = len(dictionary) #Basically assigns a number to each word \n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "if(args.action=='Save'):\n",
    "    data=read_data()\n",
    "    dictionary,reverse_dictionary=(build_dataset(data))\n",
    "    with open('Speech_to_Character_Dictionary.txt','wb') as f:\n",
    "        pickle.dump(dictionary,f)\n",
    "    with open('Speech_to_Character_Reverse_Dictionary.txt','wb') as f:\n",
    "        pickle.dump(reverse_dictionary,f)\n",
    "elif(args.action=='Restore'):\n",
    "    with open('Speech_to_Character_Dictionary.txt','rb') as f:\n",
    "        dictionary=pickle.load(f)\n",
    "    with open('Speech_to_Character_Reverse_Dictionary.txt','rb') as f:\n",
    "        reverse_dictionary=pickle.load(f)\n",
    "print(dictionary)\n",
    "\n",
    "#print(len(dictionary))\n",
    "\n",
    "n_classes=28\n",
    "n_features=13\n",
    "#Hyperparameters\n",
    "num_layers=1\n",
    "n_hidden=100\n",
    "batch_size=1\n",
    "n_epochs=100\n",
    "\n",
    "#Target log path\n",
    "logs_path = '/tmp/tensorflow/timit_speech_recognition'\n",
    "\n",
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "def sparse_tuple_from(sequences, dtype=np.int32):\n",
    "    \"\"\"Create a sparse representention of x.\n",
    "    Args:\n",
    "        sequences: a list of lists of type dtype where each element is a sequence\n",
    "    Returns:\n",
    "        A tuple with (indices, values, shape)\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    values = []\n",
    "\n",
    "    for n, seq in enumerate(sequences):\n",
    "        indices.extend(zip([n] * len(matrix(seq)), range(len(matrix(seq)))))\n",
    "        values.extend([seq])\n",
    "\n",
    "    indices = np.asarray(indices, dtype=np.int32)\n",
    "    indices[:,[0, 1]] = indices[:,[1, 0]]\n",
    "    values = np.asarray(values, dtype=dtype)\n",
    "    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1] + 1], dtype=np.int32)\n",
    "\n",
    "    return indices, values, shape\n",
    "\n",
    "\n",
    "inputs = tf.placeholder(tf.float32,[None,None,n_features])\n",
    "target_idx = tf.placeholder(tf.int64)\n",
    "target_vals = tf.placeholder(tf.int32)\n",
    "target_shape = tf.placeholder(tf.int64)\n",
    "targets = tf.SparseTensor(target_idx, target_vals, target_shape)\n",
    "seq_len = tf.placeholder(tf.int32)\n",
    "\n",
    "# RNN output node weights and biases\n",
    "weights = {\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden,n_classes]),dtype=tf.float32) # Weights_shape = hidden_units X vocab_size\n",
    "    }\n",
    "biases = {\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]),tf.float32)\n",
    "        }\n",
    "        \n",
    "def LSTM_cell():\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, reuse=tf.get_variable_scope().reuse)\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=0.4)\n",
    "\n",
    "def RNN_Model(inputs,seq_len,weights,biases):\n",
    "    rnn_cell=LSTM_cell()\n",
    "    #rnn_cell = tf.contrib.rnn.MultiRNNCell(LSTM_cell() for i in range(num_layers))\n",
    "    outputs,_ = tf.nn.dynamic_rnn(rnn_cell,inputs,seq_len,dtype=tf.float32)\n",
    "        \n",
    "    outputs = tf.reshape(outputs,[-1,n_hidden])\n",
    "    #print(outputs)\n",
    "    logits = tf.matmul(outputs,weights['out']) + biases['out']\n",
    "        \n",
    "    with tf.name_scope('Weights'):\n",
    "        variable_summaries(weights['out'])\n",
    "    \n",
    "    with tf.name_scope('Biases'):\n",
    "        variable_summaries(biases['out'])\n",
    "    \n",
    "    with tf.name_scope('Activations'):\n",
    "        tf.summary.histogram('Activations',logits)\n",
    "        \n",
    "    logits = tf.reshape(logits,[batch_size,-1,n_classes])\n",
    "    logits = tf.transpose(logits,(1,0,2))\n",
    "    return logits\n",
    "\n",
    "logits=RNN_Model(inputs,seq_len,weights,biases)\n",
    "loss = ctc.ctc_loss(targets,logits,seq_len)\n",
    "with tf.name_scope(\"CTC_Loss\"):\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    tf.summary.scalar('CTC_Loss',cost)\n",
    "        \n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=0.005, momentum=0.9).minimize(cost)\n",
    "decoded, log_prob = ctc.ctc_beam_search_decoder(logits, seq_len)\n",
    "        \n",
    "label_error_rate = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32),\n",
    "                                              targets)) \n",
    "#computes the Levenshtein distance between sequences.\n",
    "#This operation takes variable-length sequences (hypothesis and truth), each provided as a SparseTensor, and computes the Levenshtein distance.\n",
    "# You can normalize the edit distance by length of truth by setting normalize to true.\n",
    "\n",
    "def convert_to_sequence(values):\n",
    "    result=\"\"\n",
    "    for value in values:\n",
    "        result+=reverse_dictionary[value]\n",
    "    return result\n",
    "        \n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(logs_path)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "def train():\n",
    "    with tf.Session(config=config) as sess:\n",
    "            if(args.action=='Restore'):\n",
    "                sess.run(init)\n",
    "                start_epoch=int(args.epoch)\n",
    "                list_of_files = glob.glob('.\\\\saver\\\\*') # * means all if need specific format then *.csv\n",
    "                latest_file_no=0\n",
    "                if(len(list_of_files)!=0):\n",
    "                    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "                    latest_file_no=latest_file.split(\".meta\")[0].split(\"-\")[-1].split(\".ckpt\")[0]\n",
    "                if(start_epoch>int(latest_file_no)):\n",
    "                    print (\"Starting epoch value greater than available epochs. Starting from last epoch run\")\n",
    "                    start_epoch=int(latest_file_no)\n",
    "                save_path=\".\\\\saver\\\\model-\"+str(start_epoch)+\".ckpt\"\n",
    "                saver.restore(sess,save_path)\n",
    "                print(\"Restored\")\n",
    "                print(sess.run(weights['out']))\n",
    "            elif(args.action=='Save'):\n",
    "                print(\"Starting from Epoch 0\")\n",
    "                sess.run(init)\n",
    "                start_epoch=0\n",
    "            writer.add_graph(sess.graph)\n",
    "            mfcc_files = [os.path.join(root, name) for root, dirs, files in os.walk('mfccnpyFiles') for name in files\n",
    "             if name.endswith((\".npy\"))]\n",
    "            text_targets=[os.path.join(root, name) for root, dirs, files in os.walk('Text_Targets') for name in files\n",
    "             if name.endswith((\".npy\"))]\n",
    "            for curr_epoch in range(start_epoch,n_epochs):\n",
    "                train_cost=train_ler=0\n",
    "                count_files=0\n",
    "                for file in mfcc_files:\n",
    "                    count=0\n",
    "                    mfcc_No_Suffix=os.path.splitext(file)[0]\n",
    "                    mfcc_File_Name=mfcc_No_Suffix.split('\\\\')[-1]\n",
    "                    for text in text_targets:\n",
    "                        text_No_Suffix=os.path.splitext(text)[0]\n",
    "                        text_File_Name=text_No_Suffix.split('\\\\')[-1]\n",
    "                        if(mfcc_File_Name==text_File_Name):\n",
    "                            count+=1\n",
    "                            train_inputs=np.load(\".\\\\mfccnpyFiles\\\\\"+mfcc_File_Name+'.npy')\n",
    "                            train_inputs = np.asarray(train_inputs[np.newaxis, :])\n",
    "                            train_inputs = (train_inputs - np.mean(train_inputs)) / np.std(train_inputs)\n",
    "                            train_seq_len = [train_inputs.shape[1]]\n",
    "                            #print(train_seq_len)\n",
    "                            \n",
    "                            content=str(np.load(\".\\\\Text_Targets\\\\\"+text_File_Name+'.npy'))\n",
    "                            content = io.StringIO(content).read()\n",
    "                            content = (((((content.replace(\"\\n\",\"\")).replace(\".\",\"\")).replace(\"?\", \"\")).replace('\"','')).replace(\"!\",\"\")).replace(\",\",\"\")\n",
    "                            content = (((content.replace(\":\",\"\")).replace(\"'\",\"\")).replace(\"-\",\"\")).replace(\";\",\"\")\n",
    "                            content=list(content.lower())\n",
    "                            #print(content)\n",
    "                            train_targets=[]\n",
    "                            for word in content:\n",
    "                                train_targets.append(dictionary[word])\n",
    "                            train_targets=np.array(train_targets)\n",
    "                            \n",
    "                            target_index,target_values,target_shapes = sparse_tuple_from(train_targets)\n",
    "                            feed = {inputs: train_inputs,target_idx:target_index,target_vals:target_values,target_shape:target_shapes,seq_len: train_seq_len}\n",
    "                            batch_cost, _ = sess.run([cost, optimizer], feed)\n",
    "                            \n",
    "                            train_cost += batch_cost * batch_size\n",
    "                            print('Truth:\\n' + convert_to_sequence(train_targets))\n",
    "                            print('Output:\\n' + convert_to_sequence(sess.run(decoded[0].values,feed_dict=feed)))\n",
    "                            #print(\"Decoded Values: \" ,sess.run(decoded[0].values,feed_dict=feed))\n",
    "                            train_ler += sess.run(label_error_rate, feed_dict=feed) * batch_size\n",
    "                            count_files+=1\n",
    "                            print(\"Batch Cost {0} after {1} file, in Epoch {2}\\n\".format(batch_cost,count_files,curr_epoch))\n",
    "                            if(count_files==6300):\n",
    "                                train_cost=(train_cost/6300)\n",
    "                                tf.summary.scalar('Train_Cost',cost)\n",
    "                                train_ler=(train_ler)/6300\n",
    "                                tf.summary.scalar('LER_Cost',train_ler)\n",
    "                                summary=sess.run(merged,feed)\n",
    "                                writer.add_summary(summary, curr_epoch)\n",
    "                            \n",
    "                        if(count>=1):\n",
    "                            break\n",
    "                \n",
    "                print(\" Train Cost: {0} in Epoch {1} \".format((train_cost),curr_epoch))\n",
    "                print(\" LER Cost: {0} in Epoch {1}\".format(train_ler,curr_epoch))\n",
    "                save_path=\".\\\\saver\\\\model-\"+str(curr_epoch)+\".ckpt\"\n",
    "                saver.save(sess,save_path)\n",
    "                print(\"Model saved \") \n",
    "\n",
    "if __name__=='__main__':\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
